{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Connect to Drive Files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#root_path = '/content/gdrive/MyDrive/CorpusData'  #change dir to your project folder "
      ],
      "metadata": {
        "id": "i4nKYXTEccoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj6gmAWmhqtz"
      },
      "source": [
        "(a) Import the required liberaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0ZjWJURhkWe"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "import requests\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "from numpy import argmax\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Read text from the source"
      ],
      "metadata": {
        "id": "QI3J1aXdemBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "        # open the file\n",
        "        file = open(filename, mode='rt', encoding='utf-8')\n",
        "        \n",
        "        # read all text\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        return text\n",
        "# Let’s define another function to split the text into English-German pairs separated by ‘\\n’.\n",
        "# We’ll then split these pairs into English sentences and Afaan Oromo sentences respectively.\n",
        "# split a text into sentences\n",
        "\n",
        "def to_lines(text):\n",
        "      sents = text.strip().split('\\n')\n",
        "      sents = [i.split('\\t') for i in sents]\n",
        "      return sents\n",
        "\n",
        "#We can now use these functions to read the text into an array in our desired format.\n",
        "\n",
        "data = read_text('/content/gdrive/MyDrive/eng_oro_25k.txt')\n",
        "oro_eng = to_lines(data)\n",
        "#deu_eng = array(deu_eng)\n",
        "\n",
        "print(data[0:200])"
      ],
      "metadata": {
        "id": "6YamenhJegUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoTXkGawioNy"
      },
      "source": [
        "---\n",
        "(c) Load the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af5mGp-DiuXi"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        " \n",
        "# split a loaded document into sentence pairs\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t')[:2] for line in  lines]\n",
        "\treturn pairs\n",
        " \n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\tline = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\tline = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        " \n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        " \n",
        "# load dataset\n",
        "filename = '/content/gdrive/MyDrive/eng_oro_25k.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-oromo.txt')\n",
        "# spot check\n",
        "for i in range(20):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_text('/content/gdrive/MyDrive/eng_oro_25k.txt')\n",
        "eng_oro = to_lines(data)\n",
        "eng_oro = array(eng_oro)\n",
        "\n",
        "eng_oro = eng_oro[:25000,:]\n",
        "eng_oro"
      ],
      "metadata": {
        "id": "jhTeP4z2jXtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# empty lists\n",
        "eng_l = []\n",
        "oro_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in eng_oro[:,0]:\n",
        "      eng_l.append(len(i.split()))\n",
        "\n",
        "for i in eng_oro[:,1]:\n",
        "      oro_l.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'English':eng_l, 'Oromo':oro_l})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fdy4l0zvjsEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjuUj7r6kZlp"
      },
      "source": [
        "\n",
        "(d) Prepare the data for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hAqk7SSkhfI"
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        " \n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-oromo.txt')\n",
        " \n",
        "# reduce dataset size to speed up training in demonstration\n",
        "n_sentences = 25000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "\n",
        "# split into train/test\n",
        "ntest=dataset.shape[0]//10\n",
        "train, test = dataset[:-ntest], dataset[-ntest:]\n",
        "print(train.shape,test.shape)\n",
        "\n",
        "# save\n",
        "save_clean_data(dataset, 'english-oromo-both.txt')\n",
        "save_clean_data(train, 'english-oromo-train.txt')\n",
        "save_clean_data(test, 'english-oromo-test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADizOxGHkwnI"
      },
      "source": [
        "(e) Tokenize the sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFlX5wWSk2QI"
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        " \n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        " \n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        " \n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        " \n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        " \n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-oromo-both.txt')\n",
        "train = load_clean_sentences('english-oromo-train.txt')\n",
        "test = load_clean_sentences('english-oromo-test.txt')\n",
        " \n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "\n",
        "# prepare german tokenizer\n",
        "oro_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "oro_vocab_size = len(oro_tokenizer.word_index) + 1\n",
        "oro_length = max_length(dataset[:, 1])\n",
        "print('Oromo Vocabulary Size: %d' % oro_vocab_size)\n",
        "print('Oromo Max Length: %d' % (oro_length))\n",
        " \n",
        "# prepare training data\n",
        "trainX = encode_sequences(oro_tokenizer, oro_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "\n",
        "# prepare test data\n",
        "testX = encode_sequences(oro_tokenizer, oro_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNzdWy3flTzA"
      },
      "source": [
        "---\n",
        "(f) Build the translation model. This uses an LSTM to return a sentence encoding of the source sentence, then replicates that encoding on the input to an LSTM that generates the target sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbwCFXvVlVb_"
      },
      "source": [
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\t# source word embedding\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\t# LSTMs to generate setence encoding\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\t# repeat source encoding over target sequence\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\t# LSTMs to generate target sentence\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\t# Dense network to produce distribution over target vocabulary\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# define model\n",
        "model = define_model(oro_vocab_size, eng_vocab_size,oro_length, eng_length, 512)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Zetvrkle2H"
      },
      "source": [
        "(g) Fit model to phrases. Training takes some minutes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6lNVcxplgO_"
      },
      "source": [
        "# Fit model\n",
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "# set up a checkpoint to save the model each epoch\n",
        "filename = 'model.h4'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "# train model\n",
        "history = model.fit(trainX, trainY, epochs=40, batch_size=512, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(h) Plot fitting graphs for the traine and test results "
      ],
      "metadata": {
        "id": "1j_-zMkMtzqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Train-Test loss graph')\n",
        "plt.legend(['[oro-en] Train','[oro-en] Validation'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0y87RsEutQRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Train-Test accuracy graph')\n",
        "plt.legend(['[oro-en] Train','[oro-en] Validation'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Unv5Arytour"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpWMN5J4nHl-"
      },
      "source": [
        "---\n",
        "(i) Evaluate model on both training and test sentences. It takes some minutes to run and calculate the BLEU scores. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhOJKGKKnLJN"
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "  actual, predicted = list(), list()\n",
        "  for i, source in enumerate(sources):\n",
        "    # translate encoded source text\n",
        "    source = source.reshape((1, source.shape[0]))\n",
        "    translation = predict_sequence(model, eng_tokenizer, source)\n",
        "    raw_target, raw_src = raw_dataset[i]\n",
        "    if i < 10:\n",
        "      print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "    actual.append([raw_target.split()])\n",
        "    predicted.append(translation.split())\n",
        "\n",
        "  # calculate BLEU score\n",
        "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-oromo-both.txt')\n",
        "train = load_clean_sentences('english-oromo-train.txt')\n",
        "test = load_clean_sentences('english-oromo-test.txt')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h4')\n",
        "\n",
        "# test on some training sequences\n",
        "print('********** English-Afaan Oromo train result **********')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "\n",
        "# test on some test sequences\n",
        "print('********** English-Afaan Oromo test result **********')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6IRriwanymJ"
      },
      "source": [
        "---\n",
        "(j) Experiment with different amounts of training data and different network configurations. "
      ]
    }
  ]
}